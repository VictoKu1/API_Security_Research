{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "\n",
    "https://drive.google.com/file/d/15MxHRAdwPXCENACwn8wLMkb98ZCjDeh6/view?usp=share_link\n",
    "\n",
    "**Dataset_4** - The most advance and complex dataset containing all the above and some more advanced features like API redirection, more requests types, deeper data access and more.\n",
    "```\n",
    "Dataset 4 baseline score - Label phase:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      Benign    0.91077   0.72604   0.80798      5019\n",
    "     Malware    0.79145   0.93596   0.85766      5575\n",
    "\n",
    "    accuracy                        0.83651     10594\n",
    "   macro avg    0.85111   0.83100   0.83282     10594\n",
    "weighted avg    0.84798   0.83651   0.83412     10594\n",
    "```\n",
    "\n",
    "```\n",
    "Dataset 4 baseline score - Attack type phase:\n",
    "\n",
    "                     precision    recall  f1-score   support\n",
    "\n",
    "             Benign    0.79608   0.78402   0.79000      5019\n",
    "   Cookie Injection    0.62268   0.61020   0.61638      1098\n",
    "Directory Traversal    0.81365   0.56364   0.66595       550\n",
    "              LOG4J    0.67879   0.21374   0.32511       524\n",
    "        Log Forging    0.80899   1.00000   0.89441       576\n",
    "                RCE    0.26182   0.27434   0.26793       565\n",
    "      SQL Injection    0.61308   0.77965   0.68640      1130\n",
    "                XSS    0.58773   0.66873   0.62562      1132\n",
    "\n",
    "           accuracy                        0.69813     10594\n",
    "          macro avg    0.64785   0.61179   0.60898     10594\n",
    "       weighted avg    0.70364   0.69813   0.69179     10594\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cisco - Ariel University API Security Detection Challenge 2023\n",
    "## Baseline code\n",
    "\n",
    "\n",
    "### Imports and global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, settings and first dataset view\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as ps\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "pio.templates['plotly_dark'].layout.autosize = False\n",
    "pio.templates['plotly_dark'].layout.width = 1_000\n",
    "pio.templates['plotly_dark'].layout.height = 800\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "import pickle\n",
    "\n",
    "# from ipywidgets import widgets\n",
    "\n",
    "# Set pandas to show all columns when you print a dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Global setting here you choose the dataset number and classification type for the model\n",
    "dataset_number = 4  # Options are [1, 2, 3, 4]\n",
    "test_type = 'attack_type'  # Options are ['label', 'attack_type']\n",
    "\n",
    "# Read the json and read it to a pandas dataframe object, you can change these settings\n",
    "with open(f'./dataset_{str(dataset_number)}_train.json') as file:\n",
    "    raw_ds = json.load(file)\n",
    "df = pd.json_normalize(raw_ds, max_level=2)\n",
    "\n",
    "# Shoe the first five lines of the dataframe to see if everything was read accordingly\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic dataset label arrangements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the black attack tag lines with \"Benign\" string\n",
    "df['request.Attack_Tag'] = df['request.Attack_Tag'].fillna('Benign')\n",
    "df['attack_type'] = df['request.Attack_Tag']\n",
    "\n",
    "# This function will be used in the lambda below to iterate over the label columns\n",
    "# You can use this snippet to run your own lambda on any data with the apply() method\n",
    "\n",
    "\n",
    "def categorize(row):\n",
    "    if row['request.Attack_Tag'] == 'Benign':\n",
    "        return 'Benign'\n",
    "    return 'Malware'\n",
    "\n",
    "\n",
    "df['label'] = df.apply(lambda row: categorize(row), axis=1)\n",
    "\n",
    "# After finishing the arrangements we delete the irrelevant column\n",
    "df.drop('request.Attack_Tag', axis=1, inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_nan_for_more_than_90 = []\n",
    "for col in df.columns:\n",
    "    if df[col].isna().sum() / df.shape[0] * 100 > 90:\n",
    "        list_of_nan_for_more_than_90.append(col)\n",
    "        print(f\"Column {col} has {df[col].isna().sum()} NaN values, which is {round(df[col].isna().sum() / df.shape[0] * 100, 2)}%, and has {df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all NAN columns or replace with desired string\n",
    "# This loop iterates over all of the column names which are all NaN\n",
    "for column in df.columns[df.isna().any()].tolist():\n",
    "    # df.drop(column, axis=1, inplace=True)\n",
    "    df[column] = df[column].fillna('None')\n",
    "\n",
    "# If you want to detect columns that may have only some NaN values use this:\n",
    "# df.loc[:, df.isna().any()].tolist()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect columns that have the same value for all rows and print them\n",
    "from collections import Counter\n",
    "for column in df.columns:\n",
    "    if len(Counter(df[column])) == 1:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting features for further feature extraction by choosing columns\n",
    "# Some will be \"simply\" encoded via label encoding and others with HashingVectorizer\n",
    "\n",
    "# On these headers we will run a \"simple\" BOW\n",
    "SIMPLE_HEADERS = ['request.headers.Accept-Encoding',\n",
    "                  'request.headers.Host',\n",
    "                  'request.method',\n",
    "                  'request.headers.Accept-Language',\n",
    "                  'request.headers.Sec-Fetch-Site',\n",
    "                  'request.headers.Sec-Fetch-Mode',\n",
    "                  'request.headers.Sec-Fetch-Dest',\n",
    "                  'request.headers.Sec-Fetch-User',\n",
    "                  'response.status',\n",
    "                  ]\n",
    "\n",
    "# On these headers we will run HashingVectorizer\n",
    "COMPLEX_HEADERS = ['request.headers.User-Agent',\n",
    "                   'request.headers.Set-Cookie',\n",
    "                   'request.headers.Date',\n",
    "                   'request.url',\n",
    "                   'response.headers.Content-Type',\n",
    "                   'response.body',\n",
    "                   'response.headers.Location',\n",
    "                   'request.headers.Content-Length',\n",
    "                   'request.headers.Cookie',\n",
    "                   'response.headers.Set-Cookie'\n",
    "                   ]\n",
    "\n",
    "COLUMNS_TO_REMOVE = ['request.body',\n",
    "                     'response.headers.Content-Length',\n",
    "                     'request.headers.Date',\n",
    "                     'request.headers.Accept',\n",
    "                     'request.headers.Connection',\n",
    "                     ]\n",
    "\n",
    "# This is our main preprocessing function that will iterate over all of the chosen\n",
    "# columns and run some feature extraction models\n",
    "\n",
    "\n",
    "def vectorize_df(df):\n",
    "    le = LabelEncoder()\n",
    "    h_vec = HashingVectorizer(n_features=4)\n",
    "\n",
    "    # Run LabelEncoder on the chosen features\n",
    "    for column in SIMPLE_HEADERS:\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "\n",
    "    # Run HashingVectorizer on the chosen features\n",
    "    for column in COMPLEX_HEADERS:\n",
    "        newHVec = h_vec.fit_transform(df[column])\n",
    "        df[column] = newHVec.todense()\n",
    "\n",
    "    # Remove some columns that may be needed.. (Or not, you decide)\n",
    "    for column in COLUMNS_TO_REMOVE:\n",
    "        df.drop(column, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "df = vectorize_df(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory check (For large datasets sometimes the dataframe will exceed the computers resources)\n",
    "df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the right features\n",
    "# In our example code we choose all the columns as our feature this can be the right or wrong way to approach the model, you choose.\n",
    "\n",
    "features_list = df.columns.to_list()\n",
    "features_list.remove('label')\n",
    "features_list.remove('attack_type')\n",
    "print(features_list)\n",
    "\n",
    "# Recheck all datatype before training to see we don't have any objects in our features\n",
    "# In this example our model must get features containing only numbers so we recheck to see if we missed anything during preprocessing\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOT SURE AREA\n",
    "\n",
    "##### START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature selection\n",
    "# from sklearn.feature_selection import RFECV\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# # Split the data to train and test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df[features_list], df[test_type], test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create the RFE object and compute a cross-validated score.\n",
    "# # The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "# rfecv = RFECV(estimator=LogisticRegression(), step=1, cv=StratifiedKFold(2), scoring='accuracy')\n",
    "# rfecv.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "# print('Selected features: %s' % list(X_train.columns[rfecv.support_]))\n",
    "\n",
    "# # Plot number of features VS. cross-validation scores\n",
    "# plt.figure()\n",
    "# plt.xlabel(\"Number of features selected\")\n",
    "# plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "def plot_pie_chart(df, column_name):\n",
    "    fig = px.pie(df, values=column_name, names=column_name)\n",
    "    fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie_chart(df, 'attack_type')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_after_pca_in_2D=pca.fit_transform(df[features_list].to_numpy())\n",
    "x_after_pca_in_2D = pca.fit_transform(ss.fit_transform(df[features_list].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data in 2D mapped to the types of attacks\n",
    "plt.scatter(x_after_pca_in_2D[:, 0], x_after_pca_in_2D[:, 1], c=df['label'].map({'Benign': 0, 'Cookie Injection': 1, 'Directory Traversal': 2, 'LOG4J': 3, 'Log Forging': 4, 'RCE': 5, 'SQL Injection': 6, 'XSS': 7}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(ss.fit_transform(df[features_list].to_numpy()))\n",
    "\n",
    "exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "px.area(\n",
    "    x=range(1, exp_var_cumul.shape[0] + 1),\n",
    "    y=exp_var_cumul,\n",
    "    labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"},\n",
    "    range_y=(0.98, 1.02),\n",
    "    title=\"SVD Explained Variance Ratio\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we can compress the data into 17 components without loosing any information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data train and test split preparations. Here we will insert our feature list and label list.\n",
    "# Afterwards the data will be trained and fitted on the amazing XGBoost model\n",
    "# X_Train and y_Train will be used for training\n",
    "# X_test and y_test.T will be used for over fitting checking and overall score testing\n",
    "\n",
    "# We convert the feature list to a numpy array, this is required for the model fitting\n",
    "X = df[features_list]#.to_numpy()\n",
    "\n",
    "# This column is the desired prediction we will train our model on\n",
    "y = np.stack(df[test_type])\n",
    "\n",
    "# We split the dataset to train and test according to the required ration\n",
    "# Do not change the test_size -> you can change anything else\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1765, random_state=42, stratify=y)\n",
    "\n",
    "# We print the resulted datasets and count the difference\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "counter = Counter(y)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(17)\n",
    "pca.fit(X_train)\n",
    "# x_train_pca = pca.transform(X_train)\n",
    "# x_train_pca_real = pca.transform(X_train)\n",
    "x_train_pca = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test_pca = pca.transform(X_test)\n",
    "# x_test_pca_real = pca.transform(X_test)\n",
    "x_test_pca = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.barh(range(x_train_pca.shape[1]), model.feature_importances_, align=\"center\")\n",
    "    plt.yticks(np.arange(x_train_pca.shape[1]), features_list)\n",
    "    plt.ylim([-1, x_train_pca.shape[1]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection with Random Forest Classifier\n",
    "\n",
    "rfc_fs = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfc_fs.fit(x_train_pca, y_train)\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plot_feature_importance(rfc_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection with AdaBoost Classifier\n",
    "\n",
    "abc_fs = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "abc_fs.fit(x_train_pca, y_train)\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plot_feature_importance(abc_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection with Gradient Boosting Classifier\n",
    "\n",
    "gbc_fs = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gbc_fs.fit(x_train_pca, y_train)\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plot_feature_importance(gbc_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance with Linear SVC\n",
    " \n",
    "from sklearn.svm import LinearSVC\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(x_train_pca, y_train)\n",
    "lsvc.coef_\n",
    "\n",
    "# Plot feature importance with Linear SVC\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.barh(range(x_train_pca.shape[1]), lsvc.coef_[0], align=\"center\")\n",
    "plt.yticks(np.arange(x_train_pca.shape[1]), features_list)\n",
    "plt.ylim([-1, x_train_pca.shape[1]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection with Decision Tree Classifier\n",
    "\n",
    "dtc_fs = DecisionTreeClassifier(random_state=42)\n",
    "dtc_fs.fit(x_train_pca, y_train)\n",
    "\n",
    "plot_feature_importance(dtc_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection with Extra Trees Classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etc_fs = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "etc_fs.fit(x_train_pca, y_train)\n",
    "\n",
    "plot_feature_importance(etc_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the feature ranking - Top 10\n",
    "fs_table = pd.DataFrame(columns=['Feature', 'Random Forest', 'AdaBoost', 'Gradient Boosting', 'Linear SVC', 'Decision Tree', 'Extra Trees'])\n",
    "fs_table['Feature'] = features_list\n",
    "fs_table['Random Forest'] = rfc_fs.feature_importances_\n",
    "\n",
    "fs_table['AdaBoost'] = abc_fs.feature_importances_\n",
    "fs_table['Gradient Boosting'] = gbc_fs.feature_importances_\n",
    "fs_table['Linear SVC'] = np.abs(lsvc.coef_[0])\n",
    "fs_table['Decision Tree'] = dtc_fs.feature_importances_\n",
    "fs_table['Extra Trees'] = etc_fs.feature_importances_\n",
    "\n",
    "fs_table['Mean'] = fs_table.mean(axis=1)\n",
    "fs_table.sort_values(by='Mean', ascending=False, inplace=True)\n",
    "fs_table.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_pca = x_train_pca[:, fs_table['Mean'].head(15).index]\n",
    "# x_test_pca = x_test_pca[:, fs_table['Mean'].head(15).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create the RFE object and compute a cross-validated score.\n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "rfecv = RFECV(estimator=LogisticRegression(), step=1, cv=StratifiedKFold(2), scoring='accuracy')\n",
    "rfecv.fit(x_train_pca, y_train)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()\n",
    "\n",
    "x_train_pca = rfecv.transform(x_train_pca)\n",
    "x_test_pca = rfecv.transform(x_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid_search(model, params):\n",
    "    # Create a grid search object which is used to find the best hyperparameters for the model\n",
    "    return GridSearchCV(estimator=model, param_grid=params, n_jobs=-1, verbose=3, cv=3, scoring='accuracy', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://plotly.com/python/parallel-categories-diagram/\n",
    "\n",
    "def plot_grid_search_results(results):\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    # Build parcats dimensions\n",
    "    categorical_dimensions = [c for c in df.columns if \"param_\" in c]\n",
    "\n",
    "    dimensions = [dict(values=df[label], label=label)for label in categorical_dimensions]\n",
    "\n",
    "    # Build colorscale\n",
    "    color = np.zeros(len(df), dtype='uint8')\n",
    "    colorscale = [[0, 'gray'], [1, 'blue']]\n",
    "\n",
    "    # Build figure as FigureWidget\n",
    "    fig = go.FigureWidget(data=[go.Scatter(x=df['std_test_score'], y=df['mean_test_score'], marker={'color': 'gray'}, mode='markers', selected={'marker': {'color': 'blue'}}, unselected={'marker': {'opacity': 0.3}}), go.Parcats(domain={'y': [0, 0.4]}, dimensions=dimensions,line={'colorscale': colorscale, 'cmin': 0,'cmax': 1, 'color': color, 'shape': 'hspline'})])\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=800, xaxis={'title': 'STD Score'},\n",
    "        yaxis={'title': 'Mean Score', 'domain': [0.6, 1]},\n",
    "        dragmode='lasso', hovermode='closest')\n",
    "\n",
    "    # Update color callback\n",
    "    def update_color(trace, points, state):\n",
    "        # Update scatter selection\n",
    "        fig.data[0].selectedpoints = points.point_inds\n",
    "\n",
    "        # Update parcats colors\n",
    "        new_color = np.zeros(len(df), dtype='uint8')\n",
    "        new_color[points.point_inds] = 1\n",
    "        fig.data[1].line.color = new_color\n",
    "\n",
    "    # Register callback on scatter selection...\n",
    "    fig.data[0].on_selection(update_color)\n",
    "    # and parcats click\n",
    "    fig.data[1].on_click(update_color)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model choosing and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest is an ensemble learning method that uses multiple decision trees to make predictions.\n",
    "# It is a robust and flexible model that can handle high-dimensional and complex data,\n",
    "# making it a good choice for API classification tasks.\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_param = {\n",
    "    \"n_estimators\": [150, 250, 750],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [20],\n",
    "    \"min_samples_split\": [2],\n",
    "    \"random_state\": [43],\n",
    "}\n",
    "rf_gcv = create_grid_search(rf, rf_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_grid_search_results(rf_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", rf_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = rf_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "filename = \"Model/\"+str(dataset_number)+\"/rf\"+str(dataset_number)+\"_model.sav\"\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support vector machines are a popular choice for classification tasks, including API classification.\n",
    "# They work by finding the hyperplane in a high-dimensional space that maximally\n",
    "# separates different classes of data points.\n",
    "\n",
    "svc = SVC(cache_size=500)\n",
    "svc_param = {\n",
    "    \"C\": [1, 2, 3],\n",
    "    \"kernel\": [\"poly\", \"rbf\"],\n",
    "    \"probability\": [True],\n",
    "}\n",
    "svc_gcv = create_grid_search(svc, svc_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_grid_search_results(svc_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", svc_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svc_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "filename = \"Model/\"+str(dataset_number)+\"/svc\"+str(dataset_number)+\"_model.sav\"\n",
    "pickle.dump(svc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression is a simple yet effective linear model for classification tasks.\n",
    "# It is particularly useful for predicting binary outcomes\n",
    "# (e.g., malicious vs benign) and is often used as a baseline\n",
    "# model for comparison with more complex models.\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr_param = {\n",
    "    \"C\": [1, 2, 3],\n",
    "    \"penalty\": [\"l2\"],\n",
    "}\n",
    "lr_gcv = create_grid_search(lr, lr_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_grid_search_results(lr_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", lr_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = lr_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "filename = \"Model/\"+str(dataset_number)+\"/lr\"+str(dataset_number)+\"_model.sav\"\n",
    "pickle.dump(lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN is a simple yet effective classification algorithm that works by finding the k nearest neighbors of a data point.\n",
    "# It is a non-parametric model that does not make any assumptions about the underlying data distribution.\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn_param = {\n",
    "    \"n_neighbors\": [3 ,4, 5, 7, 9, 11, 13, 15],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "}\n",
    "knn_gcv = create_grid_search(knn, knn_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_grid_search_results(knn_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", knn_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = knn_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "filename = \"Model/\"+str(dataset_number)+\"/knn\"+str(dataset_number)+\"_model.sav\"\n",
    "pickle.dump(knn, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision trees are a popular choice for classification tasks.\n",
    "# They work by recursively partitioning the feature space into smaller and smaller regions,\n",
    "# until each region contains only a single class of data points.\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt_param = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [3, 5, 7, 9, 11, 13, 15],\n",
    "    \"min_samples_split\": [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "}\n",
    "dt_gcv = create_grid_search(dt, dt_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_grid_search_results(dt_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", dt_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = dt_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "filename = \"Model/\"+str(dataset_number)+\"/dt\"+str(dataset_number)+\"_model.sav\"\n",
    "pickle.dump(dt, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost is a popular ensemble method that works by combining the predictions of multiple weak learners.\n",
    "# It is a simple way to improve the performance of a model by combining the predictions of multiple models.\n",
    "\n",
    "abc = AdaBoostClassifier()\n",
    "abc_param = {\n",
    "    \"n_estimators\": [50, 100, 150, 200, 250, 300, 350, 400, 450, 500],\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "abc_gcv = create_grid_search(abc, abc_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_grid_search_results(abc_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", abc_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = abc_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "filename = \"Model/\"+str(dataset_number)+\"/abc\"+str(dataset_number)+\"_model.sav\"\n",
    "pickle.dump(abc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting is a popular ensemble method that works by combining the predictions of multiple weak learners.\n",
    "# It is a simple way to improve the performance of a model by combining the predictions of multiple models.\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc_param = {\n",
    "    \"n_estimators\": [50, 100, 150, 200, 250, 300, 350, 400, 450, 500],\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "gbc_gcv = create_grid_search(gbc, gbc_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_grid_search_results(gbc_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", gbc_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = gbc_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "filename = \"Model/\"+str(dataset_number)+\"/gbc\"+str(dataset_number)+\"_model.sav\"\n",
    "pickle.dump(gbc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural networks are a popular choice for classification tasks.\n",
    "# They work by learning the weights of the connections between neurons in the network.\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "mlp_param = {\n",
    "    \"hidden_layer_sizes\": [(100, 100, 100), (100, 100, 100, 100), (100, 100, 100, 100, 100), (100, 100, 100, 100, 100, 100)],\n",
    "    \"activation\": [\"relu\"],\n",
    "    \"solver\": [\"adam\"],\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "    \"early_stopping\": [True],\n",
    "}\n",
    "mlp_gcv = create_grid_search(mlp, mlp_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_grid_search_results(mlp_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", mlp_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = mlp_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "filename = \"Model/\"+str(dataset_number)+\"/mlp\"+str(dataset_number)+\"_model.sav\"\n",
    "pickle.dump(mlp, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting classifier is a meta-classifier that combines the predictions of multiple classifiers.\n",
    "# It is a simple way to improve the performance of a model by combining the predictions of multiple models.\n",
    "\n",
    "vc = VotingClassifier(estimators=[(\"clf\",clf),(\"svc\",svc),(\"lr\",lr),(\"knn\",knn),(\"dt\",dt),(\"abc\",abc),(\"gbc\",gbc),(\"mlp\",mlp)])\n",
    "vc_param = {\n",
    "    \"weights\": [[1,1,1,1,1,1,1,1], [2,1,1,1,1,1,1,1], [1,2,1,1,1,1,1,1], [1,1,2,1,1,1,1,1], [1,1,1,2,1,1,1,1], [1,1,1,1,2,1,1,1], [1,1,1,1,1,2,1,1], [1,1,1,1,1,1,2,1], [1,1,1,1,1,1,1,2]],\n",
    "    \"voting\": [\"hard\", \"soft\"],\n",
    "}\n",
    "vc_gcv = create_grid_search(vc, vc_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_grid_search_results(vc_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", vc_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = vc_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "filename = \"Model/\"+str(dataset_number)+\"/vc\"+str(dataset_number)+\"_model.sav\"\n",
    "pickle.dump(vc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking classifier is a meta-classifier that combines the predictions of multiple classifiers.\n",
    "# It is a simple way to improve the performance of a model by combining the predictions of multiple models.\n",
    "\n",
    "sc = StackingClassifier(estimators=[(\"clf\",clf),(\"svc\",svc),(\"lr\",lr),(\"knn\",knn),(\"dt\",dt),(\"abc\",abc),(\"gbc\",gbc),(\"mlp\",mlp)], final_estimator=LogisticRegression())\n",
    "sc_param = {\n",
    "    \"final_estimator__C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "}\n",
    "sc_gcv = create_grid_search(sc, sc_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print size of x_train_pca\n",
    "print(x_train_pca.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_grid_search_results(sc_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", sc_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = sc_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "filename = \"Model/\"+str(dataset_number)+\"/sc\"+str(dataset_number)+\"_model.sav\"\n",
    "pickle.dump(sc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best model\n",
    "\n",
    "models = [clf,svc,lr,knn,dt,abc,gbc,mlp,vc,sc]\n",
    "model_names = [\"Random Forest\",\"SVM\",\"Logistic Regression\",\"KNN\",\"Decision Tree\",\"AdaBoost\",\"Gradient Boosting\",\"Neural Network\",\"Voting Classifier\",\"Stacking Classifier\"]\n",
    "model_scores = []\n",
    "\n",
    "for model in models:\n",
    "    predictions = model.predict(x_test_pca)\n",
    "    true_labels = y_test\n",
    "    model_report = classification_report(true_labels, predictions, digits=5)\n",
    "    model_scores.append(model_report.split()[12])\n",
    "\n",
    "best_model = models[model_scores.index(max(model_scores))]\n",
    "best_model_name = model_names[model_scores.index(max(model_scores))]\n",
    "best_model_score = max(model_scores)\n",
    "print(\"Best model: \" + best_model_name)\n",
    "print(\"Score: \" + str(best_model_score))\n",
    "model_report = classification_report(true_labels, predictions, digits=5)\n",
    "print(model_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result printout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(model):\n",
    "    # We print our results\n",
    "    sns.set(rc={'figure.figsize': (15, 8)})\n",
    "    predictions = model.predict(x_test_pca)\n",
    "    true_labels = y_test\n",
    "    cf_matrix = confusion_matrix(true_labels, predictions)\n",
    "    model_report = classification_report(true_labels, predictions, digits=5)\n",
    "    heatmap = sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='g', xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels))\n",
    "\n",
    "    # The heatmap is cool but this is the most important result\n",
    "    print(model_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it's your turn, use the model you have just created :)\n",
    "\n",
    "# Read the valuation json, preprocess it and run your model\n",
    "with open(f'./dataset_{str(dataset_number)}_val.json') as file:\n",
    "    raw_ds = json.load(file)\n",
    "test_df = pd.json_normalize(raw_ds, max_level=2)\n",
    "\n",
    "# Preprocess the validation dataset, remember that here you don't have the labels\n",
    "# Remove all NAN columns or replace with desired string\n",
    "# This loop iterates over all of the column names which are all NaN\n",
    "for column in test_df.columns[test_df.isna().any()].tolist():\n",
    "    test_df[column] = test_df[column].fillna('None')\n",
    "    \n",
    "test_df = vectorize_df(test_df)\n",
    "\n",
    "X = test_df[features_list].to_numpy()\n",
    "\n",
    "X = ss.transform(X)\n",
    "X = rfecv.transform(X)\n",
    "predictions = clf.predict(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your preditions\n",
    "enc = LabelEncoder()\n",
    "np.savetxt(f'./dataset_{str(dataset_number)}_{test_type}_result.txt', enc.fit_transform(predictions), fmt='%2d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
