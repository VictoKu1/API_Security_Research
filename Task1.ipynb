{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/VictoKu1/API_Security_Research/blob/master/Task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cisco - Ariel University API Security Detection Challenge 2023\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "0qy3f4gTtupa"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuMEXBTxtQX4"
   },
   "source": [
    "## Dataset 1 (Task 1)\n",
    "\n",
    "The most basic API traffic containing the least number of attacks and endpoints. Will basically enable to have a soft start. \n",
    "\n",
    "```\n",
    "Dataset 1 baseline score:\n",
    "\n",
    "                   precision    recall  f1-score   support\n",
    "\n",
    "    Benign          0.95715   0.93922   0.99792       480\n",
    "    Malware         0.99799   0.94129   0.96881       528\n",
    "    \n",
    "    accuracy                            0.96825      1008\n",
    "    macro avg       0.96860   0.96960   0.96824      1008\n",
    "    weighted avg    0.97000   0.96825   0.96827      1008\n",
    "\n",
    "```\n",
    "\n",
    "[Link to the Dataset 1](https://drive.google.com/file/d/15MxHRAdwPXCENACwn8wLMkb98ZCjDeh6/view?usp=share_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline code"
   ],
   "metadata": {
    "id": "PBWko2gLtx3k"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_GdA948tQX6"
   },
   "source": [
    "### Imports and global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvB-vaattQX7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "outputId": "76904523-e1be-46f4-edb8-3fda00f7f10e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Imports, settings and first dataset view\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as ps\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "pio.templates['plotly_dark'].layout.autosize = False\n",
    "pio.templates['plotly_dark'].layout.width = 1_000\n",
    "pio.templates['plotly_dark'].layout.height = 800\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import pickle\n",
    "\n",
    "# from ipywidgets import widgets\n",
    "# Set pandas to show all columns when you print a dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Global setting here you choose the dataset number and classification type for the model\n",
    "dataset_number = 1   # Options are [1, 2, 3, 4]\n",
    "test_type = 'label'  # Options are ['label', 'attack_type']\n",
    "\n",
    "# Read the json and read it to a pandas dataframe object, you can change these settings\n",
    "with open(f'./dataset_{str(dataset_number)}_train.json') as file:\n",
    "    raw_ds = json.load(file)\n",
    "df_org = pd.json_normalize(raw_ds, max_level=2)\n",
    "\n",
    "# Copy data\n",
    "df = df_org.copy()\n",
    "print(\"Original Data:\", df_org.shape,\n",
    "      \"\\nCopy Data:\", df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqT5w6tStQX-"
   },
   "source": [
    "### Basic dataset label arrangements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAB0tgYstQX_",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Fill the black attack tag lines with \"Benign\" string\n",
    "df['request.Attack_Tag'] = df['request.Attack_Tag'].fillna('Benign')\n",
    "df['attack_type'] = df['request.Attack_Tag']\n",
    "\n",
    "# This function will be used in the lambda below to iterate over the label columns\n",
    "# You can use this snippet to run your own lambda on any data with the apply() method\n",
    "def categorize(row):\n",
    "    if row['request.Attack_Tag'] == 'Benign':\n",
    "        return 'Benign'\n",
    "    return 'Malware'\n",
    "\n",
    "df['label'] = df.apply(lambda row: categorize(row), axis=1)\n",
    "\n",
    "# After finishing the arrangements we delete the irrelevant column\n",
    "df.drop('request.Attack_Tag', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing Data\n"
   ],
   "metadata": {
    "id": "qZcxB32Z1gv4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WctWOumGtQX_",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "COLUMNS_TO_DROP = []\n",
    "\n",
    "for col in df.columns:\n",
    "    # More the 50% Null\n",
    "    if df[col].isna().sum()/df.shape[0]*100 > 50:\n",
    "        COLUMNS_TO_DROP.append(df[col])\n",
    "        print(f\"Column {col} has {df[col].isna().sum()} NaN values, \"\n",
    "              f\"which is {round(df[col].isna().sum() / df.shape[0] * 100, 2)}%, \"\n",
    "              f\"and has {df[col].nunique()} unique values\")\n",
    "        # df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "for col in df.columns:\n",
    "    # All rows have the same value\n",
    "    if len(Counter(df[col])) == 1:\n",
    "        COLUMNS_TO_DROP.append(df[col])\n",
    "        print(f\"Column {col} has the same value for all rows\")\n",
    "        # df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "for col in df.columns[df.isna().any()].tolist():\n",
    "    df[col] = df[col].fillna('None')\n",
    "\n",
    "print(\"Deleted:\", len(COLUMNS_TO_DROP), \" New Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# On these headers we will run a \"simple\" BOW\n",
    "SIMPLE_HEADERS = ['request.headers.Accept-Encoding',\n",
    "                  # 'request.headers.Host',\n",
    "                  'request.method',\n",
    "                  'request.headers.Accept-Language',\n",
    "                  'request.headers.Sec-Fetch-Site',\n",
    "                  'request.headers.Sec-Fetch-Mode',\n",
    "                  'request.headers.Sec-Fetch-Dest',\n",
    "                  'response.status',\n",
    "                  ]\n",
    "\n",
    "# On these headers we will run HashingVector\n",
    "COMPLEX_HEADERS = ['request.headers.User-Agent',\n",
    "                   'request.headers.Set-Cookie',\n",
    "                   'request.headers.Date',\n",
    "                   'request.url',\n",
    "                   'response.headers.Content-Type',\n",
    "                   'response.body',\n",
    "                   # 'response.headers.Location',\n",
    "                   # 'request.headers.Content-Length',\n",
    "                   # 'request.headers.Cookie',\n",
    "                   # 'response.headers.Set-Cookie'\n",
    "                   ]\n",
    "\n",
    "COLUMNS_TO_REMOVE = ['request.body',\n",
    "                     'response.headers.Content-Length',\n",
    "                     'request.headers.Date',\n",
    "                     'request.headers.Accept', ###\n",
    "                     'request.headers.Connection', ###\n",
    "                     'request.headers.Sec-Fetch-User', ###\n",
    "                     'request.headers.Cookie', #\n",
    "                     'response.headers.Location', #\n",
    "                     'request.headers.Content-Length', #\n",
    "                     'response.headers.Set-Cookie', #\n",
    "                     'request.headers.Host', ##\n",
    "                     ]\n",
    "\n",
    "def vector_df(df_):\n",
    "    le = LabelEncoder()\n",
    "    h_vec = HashingVectorizer(n_features=8)\n",
    "\n",
    "    # Run LabelEncoder on the chosen features\n",
    "    for j in SIMPLE_HEADERS:\n",
    "        print(\"1\", j)\n",
    "        df_[j] = le.fit_transform(df_[j])\n",
    "\n",
    "    # Run HashingVectorized on the chosen features\n",
    "    for j in COMPLEX_HEADERS:\n",
    "        print(\"2\", j)\n",
    "        newHVec = h_vec.fit_transform(df_[j])\n",
    "        df_[j] = newHVec.todense()\n",
    "\n",
    "    # Remove cols\n",
    "    for j in COLUMNS_TO_REMOVE:\n",
    "        print(\"3\", j)\n",
    "        df_.drop(j, axis=1, inplace=True)\n",
    "\n",
    "    return df_\n",
    "\n",
    "df = vector_df(df)\n",
    "print(\"Vector_df:\", df.shape)\n",
    "df.head()"
   ],
   "metadata": {
    "id": "l7YjI-Ksu-22",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ReUVDl3stQYB",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Choose features\n",
    "features_list = df.columns.to_list()\n",
    "features_list.remove('label')\n",
    "features_list.remove('attack_type')\n",
    "\n",
    "print(features_list)\n",
    "\n",
    "# Check type\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Umf-GuotQYD",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "pca = PCA(n_components=2)\n",
    "x_after_pca_in_2D = pca.fit_transform(ss.fit_transform(df[features_list].to_numpy()))\n",
    "\n",
    "plt.scatter(x_after_pca_in_2D[:, 0],\n",
    "            x_after_pca_in_2D[:, 1],\n",
    "            c=df['label'].map({'Benign': 0, 'Malware': 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdXLzGhhtQYE",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(ss.fit_transform(df[features_list].to_numpy()))\n",
    "pca_exp_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOpSFrpCtQYE"
   },
   "source": [
    "As we can see we can compress the data into X components without losing any information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3N0VmYUtQYE"
   },
   "source": [
    "## Train Test Split\n",
    "\n",
    "*   x_Train and y_Train will be used for _Train_\n",
    "*   x_test and y_test.T will be used for _Test_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rO8EJboItQYE",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Convert the feature list to a numpy array\n",
    "x = df[features_list]\n",
    "\n",
    "# This column is the desired prediction we'll use to train our model\n",
    "y = np.stack(df[test_type])\n",
    "\n",
    "# Split the dataset to train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,\n",
    "                                                    y, \n",
    "                                                    test_size=0.1765, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)\n",
    "\n",
    "# Print the resulted datasets \n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "# Count differences\n",
    "counter = Counter(y)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmyWULmztQYF",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.transform(x_test)\n",
    "\n",
    "pca = PCA(n_components=8)\n",
    "pca.fit(x_train)\n",
    "\n",
    "x_train_pca = x_train\n",
    "x_train_pca_ = pca.transform(x_train)\n",
    "\n",
    "x_test_pca = x_test\n",
    "x_test_pca_ = pca.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25UNKDYUtQYF",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(model):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title(\"Feature importance\")\n",
    "    plt.barh(range(x_train_pca.shape[1]), model.feature_importances_, align=\"center\")\n",
    "    plt.yticks(np.arange(x_train_pca.shape[1]), features_list)\n",
    "    plt.ylim([-1, x_train_pca.shape[1]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWN-ew_ZtQYF",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Feature selection with Random Forest Classifier\n",
    "rfc_fs = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfc_fs.fit(x_train, y_train)\n",
    "\n",
    "# Plot the feature importance of the forest\n",
    "plot_feature_importance(rfc_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7z9DDcLPtQYG",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Feature selection with AdaBoost Classifier\n",
    "abc_fs = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "abc_fs.fit(x_train, y_train)\n",
    "\n",
    "# Plot the feature importance of the forest\n",
    "plot_feature_importance(abc_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ylllY_1tQYG",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Feature selection with Gradient Boosting Classifier\n",
    "gbc_fs = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gbc_fs.fit(x_train, y_train)\n",
    "\n",
    "# Plot the feature importance of the forest\n",
    "plot_feature_importance(gbc_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0U5qsaItQYG",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Feature importance with Linear SVC\n",
    "linear_svc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(x_train_pca, y_train)\n",
    "var = linear_svc.coef_\n",
    "\n",
    "# Plot feature importance with Linear SVC\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Feature importance\")\n",
    "plt.barh(range(x_train.shape[1]), linear_svc.coef_[0], align=\"center\")\n",
    "plt.yticks(np.arange(x_train.shape[1]), features_list)\n",
    "plt.ylim([-1, x_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeEEvHxYtQYG",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Feature selection with Decision Tree Classifier\n",
    "dtc_fs = DecisionTreeClassifier(random_state=42)\n",
    "dtc_fs.fit(x_train, y_train)\n",
    "\n",
    "plot_feature_importance(dtc_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Vrv_FwitQYH",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Feature selection with Extra Trees Classifier\n",
    "etc_fs = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "etc_fs.fit(x_train, y_train)\n",
    "\n",
    "plot_feature_importance(etc_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zGLxdLNtQYH",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Print top 10 feature ranking\n",
    "fs_table = pd.DataFrame(columns=['Feature',\n",
    "                                 'Random Forest',\n",
    "                                 'AdaBoost',\n",
    "                                 'Gradient Boosting',\n",
    "                                 'Linear SVC',\n",
    "                                 'Decision Tree',\n",
    "                                 'Extra Trees'])\n",
    "fs_table['Feature'] = features_list\n",
    "fs_table['Random Forest'] = rfc_fs.feature_importances_\n",
    "fs_table['AdaBoost'] = abc_fs.feature_importances_\n",
    "fs_table['Gradient Boosting'] = gbc_fs.feature_importances_\n",
    "fs_table['Linear SVC'] = np.abs(linear_svc.coef_[0])\n",
    "fs_table['Decision Tree'] = dtc_fs.feature_importances_\n",
    "fs_table['Extra Trees'] = etc_fs.feature_importances_\n",
    "fs_table['Mean'] = fs_table.mean(axis=1)\n",
    "fs_table.sort_values(by='Mean', ascending=False, inplace=True)\n",
    "fs_table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsFQnAIZtQYH",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create the RFE object and compute a cross-validated score\n",
    "recall_scorer = make_scorer(recall_score, pos_label='Malware')\n",
    "rfecv = RFECV(estimator=LogisticRegression(),\n",
    "              step=1,\n",
    "              cv=StratifiedKFold(2),\n",
    "              # scoring=recall_scorer,\n",
    "              verbose=3,\n",
    "              n_jobs=-1)\n",
    "rfecv.fit(x_train_pca, y_train)\n",
    "\n",
    "print(\"Optimal number of features: %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number Of Features\")\n",
    "plt.ylabel(\"Cross Validation Score (nb of correct classifications)\")\n",
    "plt.errorbar(range(1, len(rfecv.cv_results_[\"mean_test_score\"]) + 1),\n",
    "             rfecv.cv_results_[\"mean_test_score\"],\n",
    "             yerr=rfecv.cv_results_[\"std_test_score\"])\n",
    "plt.show()\n",
    "\n",
    "x_train_pca = rfecv.transform(x_train_pca)\n",
    "x_test_pca = rfecv.transform(x_test_pca)\n",
    "\n",
    "print(x_train_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwkR141atQYH",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Grid search to find the best hyperParameters for the model\n",
    "def create_grid_search(model, params):\n",
    "    return GridSearchCV(estimator=model,\n",
    "                        param_grid=params,\n",
    "                        # scoring=recall_scorer,\n",
    "                        n_jobs=-1,\n",
    "                        cv=3,\n",
    "                        verbose=3,\n",
    "                        return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def model_report_presentation(model):\n",
    "    # We print our results\n",
    "    sns.set(rc={'figure.figsize': (15, 8)})\n",
    "    pred = model.predict(x_test_pca)\n",
    "    true_labels = y_test\n",
    "    cf_matrix = confusion_matrix(true_labels, pred)\n",
    "    report_model = classification_report(true_labels, pred, digits=5)\n",
    "    heatmap = sns.heatmap(cf_matrix,\n",
    "                          annot=True,\n",
    "                          cmap='Blues',\n",
    "                          fmt='g',\n",
    "                          xticklabels=np.unique(true_labels),\n",
    "                          yticklabels=np.unique(true_labels))\n",
    "\n",
    "    # The heatmap is cool but this is the most important result\n",
    "    print(report_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qv9jNo_JtQYI"
   },
   "source": [
    "### Model Choosing & Fitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random Forest\n",
    "\n",
    "Ensemble learning method that uses multiple decision trees to make predictions. \n",
    "It is a robust and flexible model that can handle high-dimensional and complex data,making it a good choice for API classification tasks."
   ],
   "metadata": {
    "id": "MZh-4aqw5nsR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "IDyPm42MtQYI",
    "outputId": "6eef204b-cf81-447c-9383-108e88b104d6",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_param = {\n",
    "    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"max_depth\": [3, 5],\n",
    "}\n",
    "rf_gcv = create_grid_search(rf, rf_param)\n",
    "rf_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", rf_gcv.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_report_presentation(rf_gcv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_hNKYBmtQYJ",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "clf = rf_gcv.best_estimator_\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/rf\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SVM - Support Vector Machines \n",
    "\n",
    "Popular choice for classification tasks, including API classification. \n",
    "They work by finding the hyperplane in a high-dimensional space that maximally separates different classes of data points."
   ],
   "metadata": {
    "id": "pUFYNbKb53Xl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKk28U_8tQYJ",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "svc = SVC(cache_size=500)\n",
    "svc_param = {\n",
    "    \"C\": [1, 2, 3],\n",
    "    \"kernel\": [\"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"probability\": [True],\n",
    "}\n",
    "svc_gcv = create_grid_search(svc, svc_param)\n",
    "svc_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", svc_gcv.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_report_presentation(svc_gcv)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hwit7qV9tQYK",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "svc = svc_gcv.best_estimator_\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/svc\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(svc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "Simple yet effective linear model for classification tasks. \n",
    "It is particularly useful for predicting binary outcomes (e.g., malicious vs benign) and is often used as a baseline model for comparison with more complex models."
   ],
   "metadata": {
    "id": "X7nsGqT66L_e"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlPFCW_3tQYK",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr_param = {\n",
    "    \"C\": [1, 2, 3, 4, 5],\n",
    "    \"penalty\": [\"none\", \"l2\"],\n",
    "}\n",
    "lr_gcv = create_grid_search(lr, lr_param)\n",
    "lr_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", lr_gcv.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_report_presentation(lr_gcv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aW6Hr9XatQYL",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "lr = lr_gcv.best_estimator_\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/lr\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### KNN\n",
    "\n",
    "Simple yet effective classification algorithm that works by finding the k nearest neighbors of a data point.\n",
    "It is a non-parametric model that does not make any assumptions about the underlying data distribution.\n"
   ],
   "metadata": {
    "id": "ShxFM2lT6aoS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erKztQB5tQYL",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn_param = {\n",
    "    \"n_neighbors\": [3, 5, 9, 13],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "}\n",
    "knn_gcv = create_grid_search(knn, knn_param)\n",
    "knn_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", knn_gcv.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_report_presentation(knn_gcv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyorE_PntQYM",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "knn = knn_gcv.best_estimator_\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/knn\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(knn, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Decision Trees\n",
    "\n",
    "Popular choice for classification tasks.\n",
    "They work by recursively partitioning the feature space into smaller and smaller regions, until each region contains only a single class of data points.\n"
   ],
   "metadata": {
    "id": "1Qr6pUrk6ljz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rj8eI5NitQYM",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt_param = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [5, 7, 9],\n",
    "}\n",
    "dt_gcv = create_grid_search(dt, dt_param)\n",
    "dt_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", dt_gcv.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_report_presentation(dt_gcv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-rX4NAktQYO",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "dt = dt_gcv.best_estimator_\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/dt\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(dt, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### AdaBoost\n",
    "\n",
    "Popular ensemble method that works by combining the predictions of multiple weak learners.\n",
    "It is a simple way to improve the performance of a model by combining the predictions of multiple models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xpG7iRstQYO",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier()\n",
    "abc_param = {\n",
    "    \"n_estimators\": [3, 5, 10],\n",
    "    \"learning_rate\": [1, 100, 1000],\n",
    "}\n",
    "abc_gcv = create_grid_search(abc, abc_param)\n",
    "abc_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", abc_gcv.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_report_presentation(abc_gcv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLca5q90tQYP",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "abc = abc_gcv.best_estimator_\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/abc\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(abc, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Gradient Boosting\n",
    "\n",
    "Popular ensemble method that works by combining the predictions of multiple weak learners.\n",
    "It is a simple way to improve the performance of a model by combining the predictions of multiple models.\n"
   ],
   "metadata": {
    "id": "qBEl87sv6wys"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRR9ar89tQYQ",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "gbc_param = {\n",
    "    \"n_estimators\": [3, 5, 10],\n",
    "    \"learning_rate\": [0.001, 0.1],\n",
    "}\n",
    "gbc_gcv = create_grid_search(gbc, gbc_param)\n",
    "gbc_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", gbc_gcv.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_report_presentation(gbc_gcv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQX4Rx6ytQYQ",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "gbc = gbc_gcv.best_estimator_\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/gbc\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(gbc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Neural Networks\n",
    "\n",
    "popular choice for classification tasks.\n",
    "They work by learning the weights of the connections between neurons in the network."
   ],
   "metadata": {
    "id": "t1n6T-c26-NA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CyJE8tctQYR",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "mlp_param = {\n",
    "    \"hidden_layer_sizes\": [(10, 50, 100, 150, 100, 50, 10),\n",
    "                           (10, 50, 100, 150, 200, 150, 100, 50, 10),\n",
    "                           (10, 50, 100, 150, 200, 250, 200, 150, 100, 50, 10),\n",
    "                           ],\n",
    "    \"activation\": [\"relu\", \"logistic\"],\n",
    "    \"solver\": [\"adam\", \"sgd\"],\n",
    "    \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "    \"shuffle\": [True], \n",
    "    \"early_stopping\": [True],\n",
    "}\n",
    "mlp_gcv = create_grid_search(mlp, mlp_param)\n",
    "mlp_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", mlp_gcv.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_report_presentation(mlp_gcv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQyzKxNztQYS",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "mlp = mlp_gcv.best_estimator_\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/mlp\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(mlp, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ExtraTreesClassifier\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "etc = ExtraTreesClassifier()\n",
    "etc_param = {\n",
    "    \"n_estimators\": [300],\n",
    "}\n",
    "etc_gcv = create_grid_search(etc, etc_param)\n",
    "etc_gcv.fit(x_train_pca, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", etc_gcv.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_report_presentation(etc_gcv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "etc = etc_gcv.best_estimator_\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/abc\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(etc, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Voting Classifier\n",
    "\n",
    "Meta-classifier that combines the predictions of multiple classifiers.\n",
    "It is a simple way to improve the performance of a model by combining the predictions of multiple models.\n"
   ],
   "metadata": {
    "id": "Wbh3lFrE7Iav"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjecOnGrtQYS",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "vc = VotingClassifier(estimators=[(\"clf\",clf),\n",
    "                                  (\"svc\",svc),\n",
    "                                  (\"lr\",lr),\n",
    "                                  (\"knn\",knn),\n",
    "                                  (\"dt\",dt),\n",
    "                                  (\"abc\",abc),\n",
    "                                  (\"gbc\",gbc),\n",
    "                                  (\"mlp\",mlp),\n",
    "                                  (\"etc\",etc)])\n",
    "vc_param = {\n",
    "    \"weights\": [[1,1,1,1,1,1,1,1,1],\n",
    "                [2,1,1,1,1,1,1,1,1],\n",
    "                [1,2,1,1,1,1,1,1,1],\n",
    "                [1,1,2,1,1,1,1,1,1],\n",
    "                [1,1,1,2,1,1,1,1,1],\n",
    "                [1,1,1,1,3,1,1,1,1],\n",
    "                [1,1,1,1,1,2,1,1,1],\n",
    "                [1,1,1,1,1,1,2,1,1],\n",
    "                [1,1,1,1,1,1,1,2,1],\n",
    "                [1,1,1,1,1,1,2,1,2]],\n",
    "    \"voting\": [\"hard\", \"soft\"],\n",
    "}\n",
    "vc_gcv = create_grid_search(vc, vc_param)\n",
    "vc_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", vc_gcv.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_report_presentation(vc_gcv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1cgfU-btQYT",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "vc = vc_gcv.best_estimator_\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/vc\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(vc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Stacking Classifier\n",
    "\n",
    "Meta-classifier that combines the predictions of multiple classifiers.\n",
    "It is a simple way to improve the performance of a model by combining the predictions of multiple models.\n"
   ],
   "metadata": {
    "id": "IdpwW8Uu7Tzr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQqM32pEtQYU",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sc = StackingClassifier(estimators=[(\"clf\",clf),\n",
    "                                    (\"svc\",svc),\n",
    "                                    (\"lr\",lr),\n",
    "                                    (\"knn\",knn),\n",
    "                                    (\"dt\",dt),\n",
    "                                    (\"abc\",abc),\n",
    "                                    (\"gbc\",gbc),\n",
    "                                    (\"mlp\",mlp),\n",
    "                                    (\"etc\",etc)],\n",
    "                        final_estimator=LogisticRegression())\n",
    "sc_param = {\"final_estimator__C\": [1, 2, 3],\n",
    "            \"cv\": [\"prefit\", None]}\n",
    "sc_gcv = create_grid_search(sc, sc_param)\n",
    "sc_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", sc_gcv.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_report_presentation(sc_gcv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkgprbsHtQYV",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "sc = sc_gcv.best_estimator_\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/sc\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(sc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Choose The Best Model\n"
   ],
   "metadata": {
    "id": "UvoCW1rA7lD7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "models = [clf, svc, lr, knn, dt, abc, gbc, mlp, vc, sc, etc]\n",
    "\n",
    "model_names = [\"Random Forest\",\n",
    "               \"SVM\",\n",
    "               \"Logistic Regression\",\n",
    "               \"KNN\",\n",
    "               \"Decision Tree\",\n",
    "               \"AdaBoost\",\n",
    "               \"Gradient Boosting\",\n",
    "               \"Neural Network\",\n",
    "               \"Voting Classifier\",\n",
    "               \"Stacking Classifier\",\n",
    "               \"Extra Trees\"]\n",
    "\n",
    "model_scores = []\n",
    "\n",
    "for model in models:\n",
    "    predictions = model.predict(x_test_pca)\n",
    "    model_report = classification_report(y_test, predictions, digits=5)\n",
    "    model_scores.append(model_report.split()[12])"
   ],
   "metadata": {
    "id": "nDQGoTlw73I2",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Result\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_model = models[model_scores.index(max(model_scores))]\n",
    "best_model_name = model_names[model_scores.index(max(model_scores))]\n",
    "best_model_score = max(model_scores)\n",
    "\n",
    "print(\"Best model:\", best_model_name)\n",
    "print(\"Score:\", str(best_model_score))\n",
    "\n",
    "model_report = classification_report(y_test, predictions, digits=5)\n",
    "print(model_report)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6It-IeetQYW"
   },
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUq4XcUAtQYW",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Read the valuation json\n",
    "\n",
    "with open(f'./dataset_{str(dataset_number)}_val.json') as file:\n",
    "    raw_ds = json.load(file)\n",
    "test_df = pd.json_normalize(raw_ds, max_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "\n",
    "# Replace NAN values\n",
    "for column in test_df.columns[test_df.isna().any()].tolist():\n",
    "    test_df[column] = test_df[column].fillna('None')\n",
    "\n",
    "test_df = vector_df(test_df)\n",
    "\n",
    "test_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run the model\n",
    "\n",
    "x = test_df[features_list].to_numpy()\n",
    "x = ss.transform(x)\n",
    "x = rfecv.transform(x)\n",
    "pred_ = vc.predict(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_ZkHSZjtQYX",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Save your predictions\n",
    "\n",
    "enc = LabelEncoder()\n",
    "np.savetxt(f'./dataset_{str(dataset_number)}_{test_type}_result.txt',\n",
    "           enc.fit_transform(pred_), fmt='%2d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}