{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/VictoKu1/API_Security_Research/blob/master/Task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cisco - Ariel University API Security Detection Challenge 2023\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "0qy3f4gTtupa"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuMEXBTxtQX4"
   },
   "source": [
    "## Dataset 1 (Task 1)\n",
    "\n",
    "The most basic API traffic containing the least number of attacks and endpoints. Will basically enable to have a soft start. \n",
    "\n",
    "```\n",
    "Dataset 1 baseline score:\n",
    "\n",
    "                   precision    recall  f1-score   support\n",
    "\n",
    "    Benign          0.95715   0.93922   0.99792       480\n",
    "    Malware         0.99799   0.94129   0.96881       528\n",
    "    \n",
    "    accuracy                            0.96825      1008\n",
    "    macro avg       0.96860   0.96960   0.96824      1008\n",
    "    weighted avg    0.97000   0.96825   0.96827      1008\n",
    "\n",
    "```\n",
    "\n",
    "[Link to the Dataset 1](https://drive.google.com/file/d/15MxHRAdwPXCENACwn8wLMkb98ZCjDeh6/view?usp=share_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline code"
   ],
   "metadata": {
    "id": "PBWko2gLtx3k"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_GdA948tQX6"
   },
   "source": [
    "### Imports and global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yvB-vaattQX7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "outputId": "76904523-e1be-46f4-edb8-3fda00f7f10e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  request.headers.Host                         request.headers.User-Agent  \\\n0       127.0.0.1:5000  Mozilla/5.0 (X11; Linux i586; rv:31.0) Gecko/2...   \n1       127.0.0.1:5000  Mozilla/5.0 (X11; OpenBSD amd64; rv:28.0) Geck...   \n2       127.0.0.1:5000  Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:24....   \n3       127.0.0.1:5000  Mozilla/5.0 (Windows NT 6.1; rv:27.3) Gecko/20...   \n4       127.0.0.1:5000  Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:25...   \n\n  request.headers.Accept-Encoding request.headers.Accept  \\\n0               gzip, deflate, br                    */*   \n1               gzip, deflate, br                    */*   \n2               gzip, deflate, br                    */*   \n3               gzip, deflate, br                    */*   \n4               gzip, deflate, br                    */*   \n\n  request.headers.Connection request.headers.Accept-Language  \\\n0                 keep-alive                           de-CH   \n1                 keep-alive                              de   \n2                 keep-alive                              de   \n3                 keep-alive                           de-CH   \n4                 keep-alive                              de   \n\n  request.headers.Sec-Fetch-Site request.headers.Sec-Fetch-Mode  \\\n0                           none                    same-origin   \n1                           none                    same-origin   \n2                           none                    same-origin   \n3                           none                    same-origin   \n4                           none                    same-origin   \n\n  request.headers.Sec-Fetch-User  \\\n0                             ?1   \n1                             ?1   \n2                             ?1   \n3                             ?1   \n4                             ?1   \n\n                      request.headers.Sec-Fetch-Dest  \\\n0                                           document   \n1                                           document   \n2                                           document   \n3                                           document   \n4  document${jndi:ldaphttps://mitsui-jyuku.mixh.j...   \n\n                          request.headers.Set-Cookie  \\\n0  ['ck=o_GpTr9HHJJuQyahzYzRI32s-1_JPvkhLtweRRmjP...   \n1  ['ck=UYrrg74Ph7dWrlCA9rgXy6qtYSwxuCZK_nBsCp-j5...   \n2  ['ck=wiMIulyT7VOEQKT-4D5ChZAjxiTdRXAYUaz_OQ3Rc...   \n3  ['ck=pQiHhJSsvZ9vw-yR8NmaTGKzqUBsn583t3Q6zWEoz...   \n4  ['ck=vGDsymCRgtDms448zxnOurIjvy46d_4aAGJRG6gmh...   \n\n            request.headers.Date request.method  \\\n0  Mon, 21 Nov 2022 18:12:16 GMT            GET   \n1  Mon, 21 Nov 2022 18:12:16 GMT            GET   \n2  Mon, 21 Nov 2022 18:12:16 GMT            GET   \n3  Mon, 21 Nov 2022 18:12:16 GMT            GET   \n4  Mon, 21 Nov 2022 18:12:16 GMT            GET   \n\n                                         request.url request.body  \\\n0  http://127.0.0.1:5000/static/download_txt/../....                \n1   http://127.0.0.1:5000/categories/check/name/2467                \n2                  http://127.0.0.1:5000/cookielogin                \n3       http://127.0.0.1:5000/states/get/region/6117                \n4   http://127.0.0.1:5000/categories/check/name/2508                \n\n    request.Attack_Tag response.status response.headers.Content-Type  \\\n0  Directory Traversal          200 OK              application/json   \n1                  NaN   404 NOT FOUND              application/json   \n2     Cookie Injection          200 OK      text/html; charset=utf-8   \n3                  NaN   404 NOT FOUND              application/json   \n4                LOG4J   404 NOT FOUND              application/json   \n\n  response.headers.Content-Length  response.status_code  \\\n0                              72                   200   \n1                              41                   404   \n2                             105                   200   \n3                              30                   404   \n4                              41                   404   \n\n                                       response.body  \\\n0  {\"error\": \"File ../../../../../../../../window...   \n1               {\"error\": \"Category name not found\"}   \n2  <h1>Logged in as Cedric</h1><form method='POST...   \n3                          {\"error\": \"Not a region\"}   \n4               {\"error\": \"Category name not found\"}   \n\n                              request.headers.Cookie  \\\n0                                                NaN   \n1                                                NaN   \n2  username=gASVyQAAAAAAAACMCGJ1aWx0aW5zlIwEZXZhb...   \n3                                                NaN   \n4                                                NaN   \n\n  response.headers.Location request.headers.Content-Length  \\\n0                       NaN                            NaN   \n1                       NaN                            NaN   \n2                       NaN                            NaN   \n3                       NaN                            NaN   \n4                       NaN                            NaN   \n\n  response.headers.Set-Cookie  \n0                         NaN  \n1                         NaN  \n2                         NaN  \n3                         NaN  \n4                         NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>request.headers.Host</th>\n      <th>request.headers.User-Agent</th>\n      <th>request.headers.Accept-Encoding</th>\n      <th>request.headers.Accept</th>\n      <th>request.headers.Connection</th>\n      <th>request.headers.Accept-Language</th>\n      <th>request.headers.Sec-Fetch-Site</th>\n      <th>request.headers.Sec-Fetch-Mode</th>\n      <th>request.headers.Sec-Fetch-User</th>\n      <th>request.headers.Sec-Fetch-Dest</th>\n      <th>request.headers.Set-Cookie</th>\n      <th>request.headers.Date</th>\n      <th>request.method</th>\n      <th>request.url</th>\n      <th>request.body</th>\n      <th>request.Attack_Tag</th>\n      <th>response.status</th>\n      <th>response.headers.Content-Type</th>\n      <th>response.headers.Content-Length</th>\n      <th>response.status_code</th>\n      <th>response.body</th>\n      <th>request.headers.Cookie</th>\n      <th>response.headers.Location</th>\n      <th>request.headers.Content-Length</th>\n      <th>response.headers.Set-Cookie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>127.0.0.1:5000</td>\n      <td>Mozilla/5.0 (X11; Linux i586; rv:31.0) Gecko/2...</td>\n      <td>gzip, deflate, br</td>\n      <td>*/*</td>\n      <td>keep-alive</td>\n      <td>de-CH</td>\n      <td>none</td>\n      <td>same-origin</td>\n      <td>?1</td>\n      <td>document</td>\n      <td>['ck=o_GpTr9HHJJuQyahzYzRI32s-1_JPvkhLtweRRmjP...</td>\n      <td>Mon, 21 Nov 2022 18:12:16 GMT</td>\n      <td>GET</td>\n      <td>http://127.0.0.1:5000/static/download_txt/../....</td>\n      <td></td>\n      <td>Directory Traversal</td>\n      <td>200 OK</td>\n      <td>application/json</td>\n      <td>72</td>\n      <td>200</td>\n      <td>{\"error\": \"File ../../../../../../../../window...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>127.0.0.1:5000</td>\n      <td>Mozilla/5.0 (X11; OpenBSD amd64; rv:28.0) Geck...</td>\n      <td>gzip, deflate, br</td>\n      <td>*/*</td>\n      <td>keep-alive</td>\n      <td>de</td>\n      <td>none</td>\n      <td>same-origin</td>\n      <td>?1</td>\n      <td>document</td>\n      <td>['ck=UYrrg74Ph7dWrlCA9rgXy6qtYSwxuCZK_nBsCp-j5...</td>\n      <td>Mon, 21 Nov 2022 18:12:16 GMT</td>\n      <td>GET</td>\n      <td>http://127.0.0.1:5000/categories/check/name/2467</td>\n      <td></td>\n      <td>NaN</td>\n      <td>404 NOT FOUND</td>\n      <td>application/json</td>\n      <td>41</td>\n      <td>404</td>\n      <td>{\"error\": \"Category name not found\"}</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>127.0.0.1:5000</td>\n      <td>Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:24....</td>\n      <td>gzip, deflate, br</td>\n      <td>*/*</td>\n      <td>keep-alive</td>\n      <td>de</td>\n      <td>none</td>\n      <td>same-origin</td>\n      <td>?1</td>\n      <td>document</td>\n      <td>['ck=wiMIulyT7VOEQKT-4D5ChZAjxiTdRXAYUaz_OQ3Rc...</td>\n      <td>Mon, 21 Nov 2022 18:12:16 GMT</td>\n      <td>GET</td>\n      <td>http://127.0.0.1:5000/cookielogin</td>\n      <td></td>\n      <td>Cookie Injection</td>\n      <td>200 OK</td>\n      <td>text/html; charset=utf-8</td>\n      <td>105</td>\n      <td>200</td>\n      <td>&lt;h1&gt;Logged in as Cedric&lt;/h1&gt;&lt;form method='POST...</td>\n      <td>username=gASVyQAAAAAAAACMCGJ1aWx0aW5zlIwEZXZhb...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>127.0.0.1:5000</td>\n      <td>Mozilla/5.0 (Windows NT 6.1; rv:27.3) Gecko/20...</td>\n      <td>gzip, deflate, br</td>\n      <td>*/*</td>\n      <td>keep-alive</td>\n      <td>de-CH</td>\n      <td>none</td>\n      <td>same-origin</td>\n      <td>?1</td>\n      <td>document</td>\n      <td>['ck=pQiHhJSsvZ9vw-yR8NmaTGKzqUBsn583t3Q6zWEoz...</td>\n      <td>Mon, 21 Nov 2022 18:12:16 GMT</td>\n      <td>GET</td>\n      <td>http://127.0.0.1:5000/states/get/region/6117</td>\n      <td></td>\n      <td>NaN</td>\n      <td>404 NOT FOUND</td>\n      <td>application/json</td>\n      <td>30</td>\n      <td>404</td>\n      <td>{\"error\": \"Not a region\"}</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>127.0.0.1:5000</td>\n      <td>Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:25...</td>\n      <td>gzip, deflate, br</td>\n      <td>*/*</td>\n      <td>keep-alive</td>\n      <td>de</td>\n      <td>none</td>\n      <td>same-origin</td>\n      <td>?1</td>\n      <td>document${jndi:ldaphttps://mitsui-jyuku.mixh.j...</td>\n      <td>['ck=vGDsymCRgtDms448zxnOurIjvy46d_4aAGJRG6gmh...</td>\n      <td>Mon, 21 Nov 2022 18:12:16 GMT</td>\n      <td>GET</td>\n      <td>http://127.0.0.1:5000/categories/check/name/2508</td>\n      <td></td>\n      <td>LOG4J</td>\n      <td>404 NOT FOUND</td>\n      <td>application/json</td>\n      <td>41</td>\n      <td>404</td>\n      <td>{\"error\": \"Category name not found\"}</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports, settings and first dataset view\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as ps\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "pio.templates['plotly_dark'].layout.autosize = False\n",
    "pio.templates['plotly_dark'].layout.width = 1_000\n",
    "pio.templates['plotly_dark'].layout.height = 800\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import pickle\n",
    "\n",
    "# from ipywidgets import widgets\n",
    "# Set pandas to show all columns when you print a dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Global setting here you choose the dataset number and classification type for the model\n",
    "dataset_number = 1   # Options are [1, 2, 3, 4]\n",
    "test_type = 'label'  # Options are ['label', 'attack_type']\n",
    "\n",
    "# Read the json and read it to a pandas dataframe object, you can change these settings\n",
    "with open(f'./dataset_{str(dataset_number)}_train.json') as file:\n",
    "    raw_ds = json.load(file)\n",
    "df = pd.json_normalize(raw_ds, max_level=2)\n",
    "\n",
    "# Shoe the first five lines of the dataframe to see if everything was read accordingly\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqT5w6tStQX-"
   },
   "source": [
    "### Basic dataset label arrangements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yAB0tgYstQX_"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'request.Attack_Tag'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\API_Security_Research\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3802\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3803\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3804\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32m~\\PycharmProjects\\API_Security_Research\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\API_Security_Research\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'request.Attack_Tag'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Fill the black attack tag lines with \"Benign\" string\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrequest.Attack_Tag\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrequest.Attack_Tag\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mfillna(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBenign\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattack_type\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrequest.Attack_Tag\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# This function will be used in the lambda below to iterate over the label columns\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# You can use this snippet to run your own lambda on any data with the apply() method\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\API_Security_Research\\venv\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3803\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3804\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3805\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3806\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3807\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\PycharmProjects\\API_Security_Research\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3803\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[0;32m   3804\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m-> 3805\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3807\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3808\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3809\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3810\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'request.Attack_Tag'"
     ]
    }
   ],
   "source": [
    "# Fill the black attack tag lines with \"Benign\" string\n",
    "df['request.Attack_Tag'] = df['request.Attack_Tag'].fillna('Benign')\n",
    "df['attack_type'] = df['request.Attack_Tag']\n",
    "\n",
    "# This function will be used in the lambda below to iterate over the label columns\n",
    "# You can use this snippet to run your own lambda on any data with the apply() method\n",
    "def categorize(row):\n",
    "    if row['request.Attack_Tag'] == 'Benign':\n",
    "        return 'Benign'\n",
    "    return 'Malware'\n",
    "\n",
    "\n",
    "df['label'] = df.apply(lambda row: categorize(row), axis=1)\n",
    "\n",
    "# After finishing the arrangements we delete the irrelevant column\n",
    "df.drop('request.Attack_Tag', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-McMOIitQX_"
   },
   "outputs": [],
   "source": [
    "# Information about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing Data"
   ],
   "metadata": {
    "id": "qZcxB32Z1gv4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WctWOumGtQX_"
   },
   "outputs": [],
   "source": [
    "list_of_nan_for_more_than_90 = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].isna().sum() / df.shape[0] * 100 > 90:\n",
    "        list_of_nan_for_more_than_90.append(col)\n",
    "        print(f\"Column {col} has {df[col].isna().sum()} NaN values, which is {round(df[col].isna().sum() / df.shape[0] * 100, 2)}%, and has {df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YygL1wzatQYA"
   },
   "outputs": [],
   "source": [
    "# Remove all NAN columns or replace with desired string\n",
    "# This loop iterates over all of the column names which are all NaN\n",
    "\n",
    "for column in df.columns[df.isna().any()].tolist():\n",
    "    df[column] = df[column].fillna('None')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aupIL7CVtQYA"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Detect columns that have the \n",
    "# same value for all rows and print them\n",
    "for column in df.columns:\n",
    "    if len(Counter(df[column])) == 1:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p35ZEGbEtQYA"
   },
   "outputs": [],
   "source": [
    "# Setting features for further feature extraction by choosing columns\n",
    "# Some will be \"simply\" encoded via \n",
    "# label encoding and others with HashingVectorizer\n",
    "\n",
    "# On these headers we will run a \"simple\" BOW\n",
    "SIMPLE_HEADERS = ['request.headers.Accept-Encoding',\n",
    "                  'request.headers.Host',\n",
    "                  'request.method',\n",
    "                  'request.headers.Accept-Language',\n",
    "                  'request.headers.Sec-Fetch-Site',\n",
    "                  'request.headers.Sec-Fetch-Mode',\n",
    "                  'request.headers.Sec-Fetch-Dest',\n",
    "                  'response.status',\n",
    "                  ]\n",
    "\n",
    "# On these headers we will run HashingVectorizer\n",
    "COMPLEX_HEADERS = ['request.headers.User-Agent',\n",
    "                   'request.headers.Set-Cookie',\n",
    "                   'request.headers.Date',\n",
    "                   'request.url',\n",
    "                   'response.headers.Content-Type',\n",
    "                   'response.body',\n",
    "                   'response.headers.Location',\n",
    "                   'request.headers.Content-Length',\n",
    "                   'request.headers.Cookie',\n",
    "                   'response.headers.Set-Cookie'\n",
    "                   ]\n",
    "\n",
    "COLUMNS_TO_REMOVE = ['request.body',\n",
    "                     'response.headers.Content-Length',\n",
    "                     'request.headers.Date',\n",
    "                     'request.headers.Accept',\n",
    "                     'request.headers.Connection',\n",
    "                     'request.headers.Sec-Fetch-User',\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Main preprocessing function - \n",
    "# iterate over all of the chosen columns \n",
    "# and run some feature extraction models\n",
    "\n",
    "def vectorize_df(df):\n",
    "    le = LabelEncoder()\n",
    "    h_vec = HashingVectorizer(n_features=4)\n",
    "\n",
    "    # Run LabelEncoder on the chosen features\n",
    "    for column in SIMPLE_HEADERS:\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "\n",
    "    # Run HashingVectorizer on the chosen features\n",
    "    for column in COMPLEX_HEADERS:\n",
    "        newHVec = h_vec.fit_transform(df[column])\n",
    "        df[column] = newHVec.todense()\n",
    "\n",
    "    # Remove some columns that may be needed.. (Or not, you decide)\n",
    "    for column in COLUMNS_TO_REMOVE:\n",
    "        df.drop(column, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "df = vectorize_df(df)\n",
    "df.head()"
   ],
   "metadata": {
    "id": "l7YjI-Ksu-22"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1iLcF0qJtQYB"
   },
   "outputs": [],
   "source": [
    "# Memory check \n",
    "# (For large datasets sometimes the dataframe will exceed the computers resources)\n",
    "\n",
    "df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ReUVDl3stQYB"
   },
   "outputs": [],
   "source": [
    "# Choose the right features\n",
    "# In our example code we choose all the columns (feature)\n",
    "# this can be the right or wrong way to approach the model, you choose.\n",
    "\n",
    "features_list = df.columns.to_list()\n",
    "features_list.remove('label')\n",
    "features_list.remove('attack_type')\n",
    "print(features_list)\n",
    "\n",
    "# Make sure we don't have any objects in our features\n",
    "# In this example the model can get numbers only  \n",
    "# Check if we missed anything during preprocessing\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Umf-GuotQYD"
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "pca = PCA(n_components=2)\n",
    "x_after_pca_in_2D = pca.fit_transform(ss.fit_transform(df[features_list].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fuqt-9oftQYE"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x_after_pca_in_2D[:, 0], \n",
    "            x_after_pca_in_2D[:, 1], \n",
    "            c=df['label'].map({'Benign': 0, 'Malware': 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdXLzGhhtQYE"
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(ss.fit_transform(df[features_list].to_numpy()))\n",
    "\n",
    "exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "px.area(\n",
    "    x=range(1, exp_var_cumul.shape[0] + 1),\n",
    "    y=exp_var_cumul,\n",
    "    labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"},\n",
    "    range_y=(0.98, 1.02),\n",
    "    title=\"SVD Explained Variance Ratio\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOpSFrpCtQYE"
   },
   "source": [
    "As we can see we can compress the data into 14 components without losing any information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3N0VmYUtQYE"
   },
   "source": [
    "## Train Test Split\n",
    "\n",
    "*   X_Train and y_Train will be used for _Train_\n",
    "*   X_test and y_test.T will be used for _Test_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rO8EJboItQYE"
   },
   "outputs": [],
   "source": [
    "# Convert the feature list to a numpy array\n",
    "X = df[features_list]\n",
    "\n",
    "# This column is the desired prediction we'll use to train our model\n",
    "y = np.stack(df[test_type])\n",
    "\n",
    "# Split the dataset to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.1765, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)\n",
    "\n",
    "# Print the resulted datasets \n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Count differences\n",
    "counter = Counter(y)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmyWULmztQYF"
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igDYGR25tQYF"
   },
   "outputs": [],
   "source": [
    "pca = PCA(14)\n",
    "pca.fit(X_train)\n",
    "x_train_pca_real = pca.transform(X_train)\n",
    "x_train_pca = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJS_wNd3tQYF"
   },
   "outputs": [],
   "source": [
    "x_test_pca_real = pca.transform(X_test)\n",
    "x_test_pca = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25UNKDYUtQYF"
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(model):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.barh(range(x_train_pca.shape[1]), model.feature_importances_, align=\"center\")\n",
    "    plt.yticks(np.arange(x_train_pca.shape[1]), features_list)\n",
    "    plt.ylim([-1, x_train_pca.shape[1]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWN-ew_ZtQYF"
   },
   "outputs": [],
   "source": [
    "# Feature selection with Random Forest Classifier\n",
    "rfc_fs = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfc_fs.fit(x_train_pca, y_train)\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plot_feature_importance(rfc_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7z9DDcLPtQYG"
   },
   "outputs": [],
   "source": [
    "# Feature selection with AdaBoost Classifier\n",
    "abc_fs = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "abc_fs.fit(x_train_pca, y_train)\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plot_feature_importance(abc_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ylllY_1tQYG"
   },
   "outputs": [],
   "source": [
    "# Feature selection with Gradient Boosting Classifier\n",
    "gbc_fs = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gbc_fs.fit(x_train_pca, y_train)\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plot_feature_importance(gbc_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0U5qsaItQYG"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Feature importance with Linear SVC\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(x_train_pca, y_train)\n",
    "lsvc.coef_\n",
    "\n",
    "# Plot feature importance with Linear SVC\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.barh(range(x_train_pca.shape[1]), lsvc.coef_[0], align=\"center\")\n",
    "plt.yticks(np.arange(x_train_pca.shape[1]), features_list)\n",
    "plt.ylim([-1, x_train_pca.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeEEvHxYtQYG"
   },
   "outputs": [],
   "source": [
    "# Feature selection with Decision Tree Classifier\n",
    "dtc_fs = DecisionTreeClassifier(random_state=42)\n",
    "dtc_fs.fit(x_train_pca, y_train)\n",
    "\n",
    "plot_feature_importance(dtc_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Vrv_FwitQYH"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Feature selection with Extra Trees Classifier\n",
    "etc_fs = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "etc_fs.fit(x_train_pca, y_train)\n",
    "\n",
    "plot_feature_importance(etc_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zGLxdLNtQYH"
   },
   "outputs": [],
   "source": [
    "# Print the feature ranking - Top 10\n",
    "fs_table = pd.DataFrame(columns=['Feature', 'Random Forest', 'AdaBoost', 'Gradient Boosting', 'Linear SVC', 'Decision Tree', 'Extra Trees'])\n",
    "fs_table['Feature'] = features_list\n",
    "fs_table['Random Forest'] = rfc_fs.feature_importances_\n",
    "\n",
    "fs_table['AdaBoost'] = abc_fs.feature_importances_\n",
    "fs_table['Gradient Boosting'] = gbc_fs.feature_importances_\n",
    "fs_table['Linear SVC'] = np.abs(lsvc.coef_[0])\n",
    "fs_table['Decision Tree'] = dtc_fs.feature_importances_\n",
    "fs_table['Extra Trees'] = etc_fs.feature_importances_\n",
    "\n",
    "fs_table['Mean'] = fs_table.mean(axis=1)\n",
    "fs_table.sort_values(by='Mean', ascending=False, inplace=True)\n",
    "fs_table.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsFQnAIZtQYH"
   },
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create the RFE object and compute a cross-validated score.\n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "recall_scorer = make_scorer(recall_score, pos_label='Malware')\n",
    "rfecv = RFECV(estimator=LogisticRegression(), step=1, cv=StratifiedKFold(2), scoring=recall_scorer, verbose=1, n_jobs=-1)\n",
    "rfecv.fit(x_train_pca, y_train)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()\n",
    "\n",
    "x_train_pca = rfecv.transform(x_train_pca)\n",
    "x_test_pca = rfecv.transform(x_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwkR141atQYH"
   },
   "outputs": [],
   "source": [
    "def create_grid_search(model, params):\n",
    "    # Create a grid search object which is used to find the best hyperparameters for the model\n",
    "    return GridSearchCV(estimator=model, param_grid=params, n_jobs=-1, verbose=3, cv=3, coring=recall_scorer, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfbTrzEctQYH"
   },
   "outputs": [],
   "source": [
    "# https://plotly.com/python/parallel-categories-diagram/\n",
    "\n",
    "def plot_grid_search_results(results):\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Build parcats dimensions\n",
    "    categorical_dimensions = [c for c in df.columns if \"param_\" in c]\n",
    "\n",
    "    dimensions = [dict(values=df[label], label=label)for label in categorical_dimensions]\n",
    "\n",
    "    # Build colorscale\n",
    "    color = np.zeros(len(df), dtype='uint8')\n",
    "    colorscale = [[0, 'gray'], [1, 'blue']]\n",
    "\n",
    "    # Build figure as FigureWidget\n",
    "    fig = go.FigureWidget(data=[go.Scatter(x=df['std_test_score'], y=df['mean_test_score'], marker={'color': 'gray'}, mode='markers', selected={'marker': {'color': 'blue'}}, unselected={'marker': {'opacity': 0.3}}), go.Parcats(domain={'y': [0, 0.4]}, dimensions=dimensions,line={'colorscale': colorscale, 'cmin': 0,'cmax': 1, 'color': color, 'shape': 'hspline'})])\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=800, xaxis={'title': 'STD Score'},\n",
    "        yaxis={'title': 'Mean Score', 'domain': [0.6, 1]},\n",
    "        dragmode='lasso', hovermode='closest')\n",
    "\n",
    "    # Update color callback\n",
    "    def update_color(trace, points, state):\n",
    "        # Update scatter selection\n",
    "        fig.data[0].selectedpoints = points.point_inds\n",
    "\n",
    "        # Update parcats colors\n",
    "        new_color = np.zeros(len(df), dtype='uint8')\n",
    "        new_color[points.point_inds] = 1\n",
    "        fig.data[1].line.color = new_color\n",
    "\n",
    "    # Register callback on scatter selection...\n",
    "    fig.data[0].on_selection(update_color)\n",
    "    # and parcats click\n",
    "    fig.data[1].on_click(update_color)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qv9jNo_JtQYI"
   },
   "source": [
    "### Model Choosing & Fitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random Forest\n",
    "\n",
    "Ensemble learning method that uses multiple decision trees to make predictions. \n",
    "It is a robust and flexible model that can handle high-dimensional and complex data,making it a good choice for API classification tasks."
   ],
   "metadata": {
    "id": "MZh-4aqw5nsR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "IDyPm42MtQYI",
    "outputId": "6eef204b-cf81-447c-9383-108e88b104d6"
   },
   "outputs": [],
   "source": [
    "# Random forest is an ensemble learning method that uses multiple decision trees to make predictions.\n",
    "# It is a robust and flexible model that can handle high-dimensional and complex data,\n",
    "# making it a good choice for API classification tasks.\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_param = {\n",
    "    \"n_estimators\": [150, 250, 750],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [20],\n",
    "    \"min_samples_split\": [2],\n",
    "    \"random_state\": [43],\n",
    "}\n",
    "\n",
    "rf_gcv = create_grid_search(rf, rf_param)\n",
    "rf_gcv.fit(x_train_pca, y_train)\n",
    "\n",
    "print(\"Best parameters: \", rf_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-E1wNAStQYI"
   },
   "outputs": [],
   "source": [
    "clf = rf_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_hNKYBmtQYJ"
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/rf\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXlrw-4xtQYJ"
   },
   "outputs": [],
   "source": [
    "# show(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SVM - Support Vector Machines \n",
    "\n",
    "Popular choice for classification tasks, including API classification. \n",
    "They work by finding the hyperplane in a high-dimensional space that maximally separates different classes of data points."
   ],
   "metadata": {
    "id": "pUFYNbKb53Xl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKk28U_8tQYJ"
   },
   "outputs": [],
   "source": [
    "svc = SVC(cache_size=500)\n",
    "svc_param = {\n",
    "    \"C\": [1, 2, 3],\n",
    "    \"kernel\": [\"poly\", \"rbf\"],\n",
    "    \"probability\": [True],\n",
    "}\n",
    "svc_gcv = create_grid_search(svc, svc_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAfK-ygWtQYJ"
   },
   "outputs": [],
   "source": [
    "svc_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9C-_szHMtQYJ"
   },
   "outputs": [],
   "source": [
    "# plot_grid_search_results(svc_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxl010CmtQYJ"
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", svc_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQbjteT6tQYK"
   },
   "outputs": [],
   "source": [
    "svc = svc_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hwit7qV9tQYK"
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/svc\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(svc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKfJ0Bt4tQYK"
   },
   "outputs": [],
   "source": [
    "# show(svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "Simple yet effective linear model for classification tasks. \n",
    "It is particularly useful for predicting binary outcomes (e.g., malicious vs benign) and is often used as a baseline model for comparison with more complex models."
   ],
   "metadata": {
    "id": "X7nsGqT66L_e"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlPFCW_3tQYK"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr_param = {\n",
    "    \"C\": [1, 2, 3],\n",
    "    \"penalty\": [\"l2\"],\n",
    "}\n",
    "lr_gcv = create_grid_search(lr, lr_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3ucjnPotQYK"
   },
   "outputs": [],
   "source": [
    "lr_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORRdVD-vtQYK"
   },
   "outputs": [],
   "source": [
    "# plot_grid_search_results(lr_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQtoXrXYtQYK"
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", lr_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIC5E8XmtQYL"
   },
   "outputs": [],
   "source": [
    "lr = lr_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aW6Hr9XatQYL"
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/lr\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "in6bIMJttQYL"
   },
   "outputs": [],
   "source": [
    "# show(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### KNN\n",
    "\n",
    "Simple yet effective classification algorithm that works by finding the k nearest neighbors of a data point.\n",
    "It is a non-parametric model that does not make any assumptions about the underlying data distribution.\n"
   ],
   "metadata": {
    "id": "ShxFM2lT6aoS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erKztQB5tQYL"
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn_param = {\n",
    "    \"n_neighbors\": [3 ,4, 5, 7, 9, 11, 13, 15],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "}\n",
    "knn_gcv = create_grid_search(knn, knn_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYeuRBEutQYL"
   },
   "outputs": [],
   "source": [
    "knn_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PO7fmze6tQYL"
   },
   "outputs": [],
   "source": [
    "# plot_grid_search_results(knn_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqidZrUZtQYM"
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", knn_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CcHklyXVtQYM"
   },
   "outputs": [],
   "source": [
    "knn = knn_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyorE_PntQYM"
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/knn\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(knn, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUffmTtItQYM"
   },
   "outputs": [],
   "source": [
    "# show(knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Decision Trees\n",
    "\n",
    "Popular choice for classification tasks.\n",
    "They work by recursively partitioning the feature space into smaller and smaller regions, until each region contains only a single class of data points.\n"
   ],
   "metadata": {
    "id": "1Qr6pUrk6ljz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rj8eI5NitQYM"
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt_param = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [3, 5, 7, 9, 11, 13, 15],\n",
    "    \"min_samples_split\": [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "}\n",
    "dt_gcv = create_grid_search(dt, dt_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEUxHp0ZtQYN"
   },
   "outputs": [],
   "source": [
    "dt_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzxzSunGtQYN"
   },
   "outputs": [],
   "source": [
    "# plot_grid_search_results(dt_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsXYaRVitQYN"
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", dt_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nbb1qQ8ftQYN"
   },
   "outputs": [],
   "source": [
    "dt = dt_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-rX4NAktQYO"
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/dt\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(dt, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2h1tgC9ctQYO"
   },
   "outputs": [],
   "source": [
    "# show(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xpG7iRstQYO"
   },
   "outputs": [],
   "source": [
    "# AdaBoost is a popular ensemble method that works by combining the predictions of multiple weak learners.\n",
    "# It is a simple way to improve the performance of a model by combining the predictions of multiple models.\n",
    "\n",
    "abc = AdaBoostClassifier()\n",
    "abc_param = {\n",
    "    \"n_estimators\": [50, 100, 150, 200, 250, 300, 350, 400, 450, 500],\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "abc_gcv = create_grid_search(abc, abc_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bbSfpjOtQYO"
   },
   "outputs": [],
   "source": [
    "abc_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBZlBeZztQYO"
   },
   "outputs": [],
   "source": [
    "# plot_grid_search_results(abc_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2J_pTDDCtQYO"
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", abc_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUxoeGw4tQYP"
   },
   "outputs": [],
   "source": [
    "abc = abc_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLca5q90tQYP"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "filename = \"Model/\"+str(dataset_number)+\"/abc\"+str(dataset_number)+\"_model.sav\"\n",
    "pickle.dump(abc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wrihZKatQYP"
   },
   "outputs": [],
   "source": [
    "# show(abc)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Gradient Boosting\n",
    "\n",
    "Popular ensemble method that works by combining the predictions of multiple weak learners.\n",
    "It is a simple way to improve the performance of a model by combining the predictions of multiple models.\n"
   ],
   "metadata": {
    "id": "qBEl87sv6wys"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRR9ar89tQYQ"
   },
   "outputs": [],
   "source": [
    "# Gradient Boosting is a popular ensemble method that works by combining the predictions of multiple weak learners.\n",
    "# It is a simple way to improve the performance of a model by combining the predictions of multiple models.\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc_param = {\n",
    "    \"n_estimators\": [50, 100, 150, 200, 250, 300, 350, 400, 450, 500],\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "gbc_gcv = create_grid_search(gbc, gbc_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oHN7bshtQYQ"
   },
   "outputs": [],
   "source": [
    "gbc_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBzrGMNRtQYQ"
   },
   "outputs": [],
   "source": [
    "# plot_grid_search_results(gbc_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1S60wHcntQYQ"
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", gbc_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtnDEYMutQYQ"
   },
   "outputs": [],
   "source": [
    "gbc = gbc_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQX4Rx6ytQYQ"
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/gbc\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(gbc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlajiIiGtQYR"
   },
   "outputs": [],
   "source": [
    "# show(gbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Neural Networks\n",
    "\n",
    "popular choice for classification tasks.\n",
    "They work by learning the weights of the connections between neurons in the network."
   ],
   "metadata": {
    "id": "t1n6T-c26-NA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CyJE8tctQYR"
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "mlp_param = {\n",
    "    \"hidden_layer_sizes\": [(100, 100, 100), (100, 100, 100, 100), (100, 100, 100, 100, 100), (100, 100, 100, 100, 100, 100)],\n",
    "    \"activation\": [\"relu\"],\n",
    "    \"solver\": [\"adam\"],\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "    \"early_stopping\": [True],\n",
    "}\n",
    "mlp_gcv = create_grid_search(mlp, mlp_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7yhH1jQtQYR"
   },
   "outputs": [],
   "source": [
    "mlp_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXQXCrGYtQYR"
   },
   "outputs": [],
   "source": [
    "# plot_grid_search_results(mlp_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFAirvZ4tQYR"
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", mlp_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3s5owMc3tQYS"
   },
   "outputs": [],
   "source": [
    "mlp = mlp_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQyzKxNztQYS"
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/mlp\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(mlp, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65LKnacBtQYS"
   },
   "outputs": [],
   "source": [
    "# show(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Voting Classifier\n",
    "\n",
    "Meta-classifier that combines the predictions of multiple classifiers.\n",
    "It is a simple way to improve the performance of a model by combining the predictions of multiple models.\n"
   ],
   "metadata": {
    "id": "Wbh3lFrE7Iav"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjecOnGrtQYS"
   },
   "outputs": [],
   "source": [
    "vc = VotingClassifier(estimators=[(\"clf\",clf),(\"svc\",svc),(\"lr\",lr),(\"knn\",knn),(\"dt\",dt),(\"abc\",abc),(\"gbc\",gbc),(\"mlp\",mlp)])\n",
    "vc_param = {\n",
    "    \"weights\": [[1,1,1,1,1,1,1,1], [2,1,1,1,1,1,1,1], [1,2,1,1,1,1,1,1], [1,1,2,1,1,1,1,1], [1,1,1,2,1,1,1,1], [1,1,1,1,2,1,1,1], [1,1,1,1,1,2,1,1], [1,1,1,1,1,1,2,1], [1,1,1,1,1,1,1,2]],\n",
    "    \"voting\": [\"hard\", \"soft\"],\n",
    "}\n",
    "vc_gcv = create_grid_search(vc, vc_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POoSyTjotQYT"
   },
   "outputs": [],
   "source": [
    "vc_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzAeBqdItQYT"
   },
   "outputs": [],
   "source": [
    "# plot_grid_search_results(vc_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdrlXOPutQYT"
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", vc_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_78ltzhtQYT"
   },
   "outputs": [],
   "source": [
    "vc = vc_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1cgfU-btQYT"
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/vc\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(vc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yra0UOZYtQYU"
   },
   "outputs": [],
   "source": [
    "# show(vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Stacking Classifier\n",
    "\n",
    "Meta-classifier that combines the predictions of multiple classifiers.\n",
    "It is a simple way to improve the performance of a model by combining the predictions of multiple models.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "IdpwW8Uu7Tzr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQqM32pEtQYU"
   },
   "outputs": [],
   "source": [
    "sc = StackingClassifier(estimators=[(\"clf\",clf),(\"svc\",svc),(\"lr\",lr),(\"knn\",knn),(\"dt\",dt),(\"abc\",abc),(\"gbc\",gbc),(\"mlp\",mlp)], final_estimator=LogisticRegression())\n",
    "sc_param = {\n",
    "    \"final_estimator__C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "}\n",
    "sc_gcv = create_grid_search(sc, sc_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hY4ptPgjtQYU"
   },
   "outputs": [],
   "source": [
    "print(x_train_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtMOq7DetQYU"
   },
   "outputs": [],
   "source": [
    "sc_gcv.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yyXccACtQYU"
   },
   "outputs": [],
   "source": [
    "# plot_grid_search_results(sc_gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzKi5rZhtQYU"
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", sc_gcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbaDAsXHtQYV"
   },
   "outputs": [],
   "source": [
    "sc = sc_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkgprbsHtQYV"
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# filename = \"Model/\"+str(dataset_number)+\"/sc\"+str(dataset_number)+\"_model.sav\"\n",
    "# pickle.dump(sc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8gUCfswtQYV"
   },
   "outputs": [],
   "source": [
    "# show(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Choose The Best Model\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "UvoCW1rA7lD7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "models = [clf, svc, lr, knn, dt, abc, gbc, mlp, vc, sc]\n",
    "\n",
    "model_names = [\"Random Forest\",\n",
    "               \"SVM\",\n",
    "               \"Logistic Regression\",\n",
    "               \"KNN\",\n",
    "               \"Decision Tree\",\n",
    "               \"AdaBoost\",\n",
    "               \"Gradient Boosting\",\n",
    "               \"Neural Network\",\n",
    "               \"Voting Classifier\",\n",
    "               \"Stacking Classifier\"]\n",
    "\n",
    "model_scores = []\n",
    "\n",
    "for model in models:\n",
    "    predictions = model.predict(x_test_pca)\n",
    "    true_labels = y_test\n",
    "    model_report = classification_report(true_labels, predictions, digits=5)\n",
    "    model_scores.append(model_report.split()[12])"
   ],
   "metadata": {
    "id": "nDQGoTlw73I2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wzy9Ek6QtQYV"
   },
   "outputs": [],
   "source": [
    "best_model = models[model_scores.index(max(model_scores))]\n",
    "best_model_name = model_names[model_scores.index(max(model_scores))]\n",
    "best_model_score = max(model_scores)\n",
    "\n",
    "print(\"Best model:\", best_model_name)\n",
    "print(\"Score:\", str(best_model_score))\n",
    "\n",
    "model_report = classification_report(true_labels, predictions, digits=5)\n",
    "\n",
    "print(model_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwjKOgastQYW"
   },
   "source": [
    "#### Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItSnLVdVtQYW"
   },
   "outputs": [],
   "source": [
    "def show(model):\n",
    "    sns.set(rc={'figure.figsize': (15, 8)})\n",
    "    predictions = model.predict(x_test_pca)\n",
    "    true_labels = y_test\n",
    "    cf_matrix = confusion_matrix(true_labels, predictions)\n",
    "    model_report = classification_report(true_labels, predictions, digits=5)\n",
    "    heatmap = sns.heatmap(cf_matrix, \n",
    "                          annot=True, \n",
    "                          cmap='Blues', \n",
    "                          fmt='g', \n",
    "                          xticklabels=np.unique(true_labels), \n",
    "                          yticklabels=np.unique(true_labels))\n",
    "\n",
    "    print(model_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6It-IeetQYW"
   },
   "source": [
    "## Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUq4XcUAtQYW"
   },
   "outputs": [],
   "source": [
    "# Read the valuation json\n",
    "# Preprocess data & run model\n",
    "with open(f'./dataset_{str(dataset_number)}_val.json') as file:\n",
    "    raw_ds = json.load(file)\n",
    "test_df = pd.json_normalize(raw_ds, max_level=2)\n",
    "\n",
    "# Preprocess the validation dataset\n",
    "# Remove / Replace all NAN columns\n",
    "for column in test_df.columns[test_df.isna().any()].tolist():\n",
    "    test_df[column] = test_df[column].fillna('None')\n",
    "    \n",
    "test_df = vectorize_df(test_df)\n",
    "\n",
    "X = test_df[features_list].to_numpy()\n",
    "X = ss.transform(X)\n",
    "X = rfecv.transform(X)\n",
    "predictions = vc.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_ZkHSZjtQYX"
   },
   "outputs": [],
   "source": [
    "# # Save your preditions\n",
    "# enc = LabelEncoder()\n",
    "# np.savetxt(f'./dataset_{str(dataset_number)}_{test_type}_result.txt', enc.fit_transform(predictions), fmt='%2d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}